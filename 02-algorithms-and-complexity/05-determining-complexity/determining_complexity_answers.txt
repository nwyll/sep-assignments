For each -- What's the Big-O of the algorithm? Submit your work and reasoning with your solution.

1. goodbye_world.rb
O(1)
Goodbye_world runs in constant time. Regardless of the value of the input, it takes the same amount of time to print to the screen.

2. find_largest.rb
O(n)
Find_largest runs in linear time. The worst case would be if the largest item was at the end of the collection which would require running through the entire collection 1 by 1 to the last value. Therefore there would be n operations where n is the length of the collection. So as the collection length n grows so does the number of operations at the same rate, n.

3. find_largest_2D_array.rb
O(n^2)
Find_largest for the matrix runs in quadratic time. The worst case again would be if the largest item was at the end of the collection in the last row, last column. And if it is a square matrix. As written, this requires running through n rows of n length 1 by 1 to the last value. So you have n operations iterating through the rows times n operations iterating over the sub array for each row.

4. numbers_recursive.rb
O(2^n)
Recursive Fibonacci runs in exponential time (not good). As written, the function will only return 0 or 1 if n = 0 or 1, respectively. These base cases execute in constant time. Otherwise two recursive calls to numbers(n-2) and numbers(n-1) are made until the previous base cases are reached. The number of recursive calls made grows at a rate of increase matching the Fibonacci sequence itself. And once the recursion reaches the base case, those calls end in the returns executing. For example, for numbers(4) will make 8 total recursive calls, with until we reach 5 base cases, which will add 5 return operations for a total of 13 operations. The chart below summarizes the number of recursive calls made, addition base case returns made and total operations for n > 2. The base cases of n = 0 and n = 1 are ignored since those are best case scenarios.

  n   |  # of recursive calls  | # of base case returns  |  total # of operations
  2   |           2            |            2            |           4
  3   |           4            |            3            |           7
  4   |           8            |            5            |          13
  5   |          14            |            8            |          22
  6   |          24            |           13            |          37
  7   |          40            |           21            |          61

As shown by the data above, for n > 2, the number of recursive calls is increasing at a rate equal to 2 times the Fibonacci sequence itself. The number of base case returns increases at a rate equal to the Fibonacci sequence. These values are added to get the total number of calls 2Fib(n) + Fib(n), should give a rate of increase for the total of 3 times Fib(n), which is what we see in the chart above. The total number of operations does indeed increase a rate 3 times the Fibonacci sequence.

In terms of complexity calculations, coefficients can be factored out since the definition of Big-O is the lower bound k * O(f(n)). So 0(3Fib(n)) = 3 * O(Fib(n)) or just O(Fib(n)). The Fibonacci sequence is tightly bound by f(n) ~ 1.6^n, so we can round up and say the rate of increase will always be less then 2^n, resulting in O(2^n) for a recursive Fibonacci algorithm.

That was fun!!! (No, seriously.)

5. numbers_iterative.rb
I'm betting this one is better than the recursive ... let's see. Yep, this iterative Fibonacci runs in O(n), linear time. Much better.

What really controls the complexity here is the number of iterations performed or the while i < n-1 statement. All the other statements in the algorithm are declarations (and the final return) that run in constant time. So for a given n, n-1 iterations occur. As n increases the number of iterations increases linearly. What we really have is a constant time for the statements outside the while loop plus some constant time for each iteration => O(1) + O(n*1). But as n grows the first term is insignificant, so we just say this it's Big-O is O(n).

6. sort.rb
It's a Quick Sort. Quick sort has an average performance of O(nlog(n)). Lets see if I can match that logic ...

IN a quick sort you will set the pivot and iterate through the collection comparing to the pivot value creating two sub-arrays left and right that are less than and greater than the pivot value, respectively. Then calls are made recursively on the left and right subarray to sort them, which requires iterating through the sub-arrays. The recursive calls end when the base case of an array of size 1 is reached, because a single array is considered sorted. Iterating through the each arrays take n-time, and how many times that occurs is dependent on the level of recursion. For n = 2, one level of recursion is required, for n = 4, two levels of recursion are required, for n = 8, three levels of recursion are required, for n = 16, 4 levels for recursion are required. The levels of recursion increase in lg-time, increasing by one level each time n doubles. So the overall complexity is n * lg(n) or loglinear time.
